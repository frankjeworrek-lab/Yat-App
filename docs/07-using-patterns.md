# Patterns f√ºr eigene Apps nutzen

Dieses Dokument erkl√§rt Entwicklern, wie sie die **Architektur-Patterns** aus dieser App in ihren **eigenen Projekten** verwenden k√∂nnen.

## üéØ Zielgruppe

Du bist hier richtig, wenn du:
- ‚úÖ Eine eigene App baust (Web, CLI, Discord-Bot, etc.)
- ‚úÖ LLM-Integration brauchst
- ‚úÖ Die Pattern/Architektur dieser App nutzen willst
- ‚úÖ NICHT die komplette GUI √ºbernehmen willst

**Nicht:** Wenn du diese App nur erweitern willst ‚Üí siehe [03-provider-integration.md](./03-provider-integration.md)

---

## üèóÔ∏è Was kannst du wiederverwenden?

### 1. **Core-Komponenten (standalone)**

Die Business-Logic funktioniert **unabh√§ngig** von der UI:

```
core/
‚îú‚îÄ‚îÄ llm_manager.py        ‚Üê Provider-Orchestrierung
‚îî‚îÄ‚îÄ providers/
    ‚îú‚îÄ‚îÄ base_provider.py  ‚Üê Abstract Interface
    ‚îú‚îÄ‚îÄ types.py          ‚Üê Shared Models
    ‚îú‚îÄ‚îÄ openai_provider.py
    ‚îú‚îÄ‚îÄ anthropic_provider.py
    ‚îî‚îÄ‚îÄ ...
```

**Diese kannst du 1:1 in eigene Apps kopieren!**

---

### 2. **Architektur-Patterns (Konzepte)**

Selbst wenn du nicht Python nutzt, kannst du die **Patterns** √ºbernehmen:

- **Plugin-System** (Provider als Plugins)
- **Abstract Base Class** Pattern
- **Dependency Injection**
- **Strategy Pattern** (w√§hlbare Algorithmen)
- **Adapter Pattern** (verschiedene APIs ‚Üí einheitliches Interface)

---

## üì¶ Szenario 1: Core-Logik in eigener App nutzen

**Use Case:** Du baust eine Web-App (Flask/FastAPI), willst aber die Provider-Logik wiederverwenden.

### Schritt 1: Core kopieren

```bash
# In deinem Projekt
mkdir my_web_app/llm_core
cp -r ki_chat_pattern/core/* my_web_app/llm_core/
```

### Schritt 2: Standalone nutzen

```python
# my_web_app/backend.py

from llm_core.llm_manager import LLMManager
from llm_core.providers.openai_provider import OpenAIProvider
from llm_core.providers.types import ProviderConfig, Message, Role

# Setup (einmalig beim App-Start)
llm_manager = LLMManager()

openai_config = ProviderConfig(name="OpenAI", api_key="sk-...")
openai_provider = OpenAIProvider(openai_config)
await openai_provider.initialize()
llm_manager.register_provider("openai", openai_provider)

llm_manager.active_provider_id = "openai"
llm_manager.active_model_id = "gpt-4"

# In deinem Endpoint
@app.post("/chat")
async def chat(request):
    user_msg = Message(role=Role.USER, content=request.json['message'])
    history = [user_msg]  # Oder aus Session laden
    
    response = ""
    async for chunk in llm_manager.stream_chat(history):
        response += chunk
        # Optional: per WebSocket streamen
    
    return {"response": response}
```

**Fertig!** Du nutzt jetzt die gleiche Provider-Infrastruktur.

---

## üåê Szenario 2: Web-App mit FastAPI

**Komplettes Beispiel:** Web-Chat mit dieser Core-Logic

### Dateistruktur

```
my_chat_web/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # FastAPI App
‚îÇ   ‚îú‚îÄ‚îÄ websocket_handler.py # Streaming
‚îÇ   ‚îî‚îÄ‚îÄ llm_core/            # Kopiert von ki_chat_pattern
‚îÇ       ‚îú‚îÄ‚îÄ llm_manager.py
‚îÇ       ‚îî‚îÄ‚îÄ providers/
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ index.html
‚îÇ   ‚îî‚îÄ‚îÄ chat.js
‚îî‚îÄ‚îÄ requirements.txt
```

### Backend (FastAPI)

```python
# backend/main.py

from fastapi import FastAPI, WebSocket
from llm_core.llm_manager import LLMManager
from llm_core.providers.openai_provider import OpenAIProvider
from llm_core.providers.types import ProviderConfig, Message, Role
import os

app = FastAPI()

# Globaler LLMManager
llm_manager = LLMManager()

@app.on_event("startup")
async def setup_providers():
    # OpenAI
    openai_config = ProviderConfig(
        name="OpenAI",
        api_key=os.getenv("OPENAI_API_KEY")
    )
    openai_provider = OpenAIProvider(openai_config)
    await openai_provider.initialize()
    llm_manager.register_provider("openai", openai_provider)
    
    llm_manager.active_provider_id = "openai"
    llm_manager.active_model_id = "gpt-4"

@app.websocket("/ws/chat")
async def websocket_chat(websocket: WebSocket):
    await websocket.accept()
    
    while True:
        # Empfange User-Message
        data = await websocket.receive_json()
        user_msg = Message(role=Role.USER, content=data['message'])
        
        # Streaming-Response
        async for chunk in llm_manager.stream_chat([user_msg]):
            await websocket.send_json({"chunk": chunk})
        
        # End-Marker
        await websocket.send_json({"done": True})
```

### Frontend (JavaScript)

```javascript
// frontend/chat.js

const ws = new WebSocket('ws://localhost:8000/ws/chat');
const chatDiv = document.getElementById('chat');
const input = document.getElementById('input');

function sendMessage() {
    const message = input.value;
    
    // User-Message anzeigen
    chatDiv.innerHTML += `<div class="user">${message}</div>`;
    
    // An Backend senden
    ws.send(JSON.stringify({message: message}));
    
    // AI-Response (leerer Container)
    const aiDiv = document.createElement('div');
    aiDiv.className = 'ai';
    chatDiv.appendChild(aiDiv);
    
    input.value = '';
}

// Empfange Streaming-Chunks
ws.onmessage = (event) => {
    const data = JSON.parse(event.data);
    
    if (data.chunk) {
        // Akkumuliere chunks in letztem AI-Div
        const aiDivs = document.querySelectorAll('.ai');
        const lastAi = aiDivs[aiDivs.length - 1];
        lastAi.textContent += data.chunk;
    }
    
    if (data.done) {
        console.log('Response complete');
    }
};
```

**Das war's!** Du hast die Core-Logic wiederverwendet, aber deine eigene UI gebaut.

---

## ü§ñ Szenario 3: Discord-Bot

**Use Case:** Discord-Bot mit Multi-Provider-Support

```python
# discord_bot.py

import discord
from discord.ext import commands
from llm_core.llm_manager import LLMManager
from llm_core.providers.openai_provider import OpenAIProvider
from llm_core.providers.anthropic_provider import AnthropicProvider
from llm_core.providers.types import ProviderConfig, Message, Role

bot = commands.Bot(command_prefix='!')
llm_manager = LLMManager()

@bot.event
async def on_ready():
    # Setup Provider
    openai = OpenAIProvider(ProviderConfig(name="OpenAI"))
    await openai.initialize()
    llm_manager.register_provider("openai", openai)
    
    anthropic = AnthropicProvider(ProviderConfig(name="Anthropic"))
    await anthropic.initialize()
    llm_manager.register_provider("anthropic", anthropic)
    
    llm_manager.active_provider_id = "openai"
    llm_manager.active_model_id = "gpt-4"
    
    print(f'{bot.user} is ready!')

@bot.command(name='chat')
async def chat(ctx, *, message: str):
    """Chat mit KI"""
    
    user_msg = Message(role=Role.USER, content=message)
    
    # Typing-Indicator
    async with ctx.typing():
        response = ""
        async for chunk in llm_manager.stream_chat([user_msg]):
            response += chunk
    
    await ctx.send(response)

@bot.command(name='model')
async def switch_model(ctx, provider: str, model: str):
    """Wechsle Provider/Model: !model openai gpt-4"""
    
    llm_manager.active_provider_id = provider
    llm_manager.active_model_id = model
    
    await ctx.send(f'Switched to {model} on {provider}')

bot.run('YOUR_DISCORD_TOKEN')
```

**Features kostenlos:**
- ‚úÖ Multi-Provider-Support (OpenAI, Claude, etc.)
- ‚úÖ Easy Provider-Wechsel (`!model anthropic claude-3-opus`)
- ‚úÖ Streaming (bot.typing)
- ‚úÖ Erweiterbar (neue Provider einfach hinzuf√ºgen)

---

## üñ•Ô∏è Szenario 4: CLI-Tool

**Use Case:** Command-Line Chat-Client

```python
# cli_chat.py

import asyncio
from llm_core.llm_manager import LLMManager
from llm_core.providers.openai_provider import OpenAIProvider
from llm_core.providers.types import ProviderConfig, Message, Role

async def main():
    # Setup
    llm_manager = LLMManager()
    
    openai = OpenAIProvider(ProviderConfig(name="OpenAI"))
    await openai.initialize()
    llm_manager.register_provider("openai", openai)
    
    llm_manager.active_provider_id = "openai"
    llm_manager.active_model_id = "gpt-4"
    
    print("Chat started. Type 'exit' to quit.\n")
    
    history = []
    
    while True:
        # User-Input
        user_input = input("You: ")
        if user_input.lower() == 'exit':
            break
        
        user_msg = Message(role=Role.USER, content=user_input)
        history.append(user_msg)
        
        # AI-Response (streaming)
        print("AI: ", end='', flush=True)
        response_content = ""
        
        async for chunk in llm_manager.stream_chat(history):
            print(chunk, end='', flush=True)
            response_content += chunk
        
        print()  # Newline
        
        # Speichere Response in History
        ai_msg = Message(role=Role.ASSISTANT, content=response_content)
        history.append(ai_msg)

if __name__ == "__main__":
    asyncio.run(main())
```

**Ergebnis:**
```bash
$ python cli_chat.py
Chat started. Type 'exit' to quit.

You: Hello!
AI: Hello! How can I help you today?

You: What's Python?
AI: Python is a high-level programming language...
```

---

## üîß Szenario 5: Nur das Pattern (andere Sprache)

**Use Case:** Du programmierst in TypeScript/Java/Go, willst aber das Pattern nutzen.

### Das Pattern (konzeptionell)

**1. Abstract Provider Interface**

```typescript
// TypeScript-Beispiel

interface LLMProvider {
  initialize(): Promise<void>;
  getModels(): Promise<ModelInfo[]>;
  streamChat(modelId: string, messages: Message[]): AsyncGenerator<string>;
  checkHealth(): Promise<boolean>;
}

// Konkrete Implementation
class OpenAIProvider implements LLMProvider {
  async initialize() {
    // OpenAI-Client setup
  }
  
  async getModels() {
    return [
      { id: "gpt-4", name: "GPT-4", provider: "OpenAI" }
    ];
  }
  
  async *streamChat(modelId: string, messages: Message[]) {
    // API-Call zu OpenAI
    for (const chunk of response) {
      yield chunk;
    }
  }
  
  async checkHealth() {
    return true;
  }
}
```

**2. Manager mit Registry**

```typescript
class LLMManager {
  private providers: Map<string, LLMProvider> = new Map();
  private activeProviderId: string | null = null;
  private activeModelId: string | null = null;
  
  registerProvider(id: string, provider: LLMProvider) {
    this.providers.set(id, provider);
    if (!this.activeProviderId) {
      this.activeProviderId = id;
    }
  }
  
  async getAllModels(): Promise<ModelInfo[]> {
    const allModels: ModelInfo[] = [];
    
    for (const [providerId, provider] of this.providers) {
      const models = await provider.getModels();
      models.forEach(m => m.providerId = providerId);
      allModels.push(...models);
    }
    
    return allModels;
  }
  
  async *streamChat(messages: Message[]): AsyncGenerator<string> {
    const provider = this.providers.get(this.activeProviderId!);
    if (!provider) throw new Error("No active provider");
    
    yield* provider.streamChat(this.activeModelId!, messages);
  }
}
```

**3. Nutzung**

```typescript
// Setup
const manager = new LLMManager();

const openai = new OpenAIProvider({ apiKey: process.env.OPENAI_API_KEY });
await openai.initialize();
manager.registerProvider("openai", openai);

// Chat
for await (const chunk of manager.streamChat(messages)) {
  console.log(chunk);
}
```

**Das Pattern funktioniert in JEDER Sprache!**

---

## üé® Pattern-Prinzipien (universell)

### 1. **Dependency Injection**

**Statt:**
```python
# Schlecht - Hardcoded
class ChatApp:
    def __init__(self):
        self.openai_client = OpenAI(...)  # Fest verdrahtet!
```

**Besser:**
```python
# Gut - Injiziert
class ChatApp:
    def __init__(self, llm_manager: LLMManager):
        self.llm_manager = llm_manager  # Flexibel!
```

**Vorteil:** Tests, Mocks, Provider-Wechsel einfach.

---

### 2. **Plugin-Registry**

**Pattern:**
```
Manager
  ‚îî‚îÄ Registry (Dict/Map)
      ‚îú‚îÄ "openai" ‚Üí OpenAIProvider
      ‚îú‚îÄ "anthropic" ‚Üí AnthropicProvider
      ‚îî‚îÄ "custom" ‚Üí CustomProvider
```

**Vorteile:**
- Neue Plugins zur Laufzeit hinzuf√ºgen
- Kein Recompile n√∂tig
- Plugins isoliert

---

### 3. **Strategy Pattern**

Verschiedene Algorithmen (Provider) austauschbar:

```python
# Gleicher Code f√ºr alle Provider
async for chunk in manager.stream_chat(messages):
    # Funktioniert mit OpenAI, Anthropic, etc.
    print(chunk)
```

**Kein if/else basierend auf Provider-Typ!**

---

### 4. **Adapter Pattern**

Verschiedene APIs ‚Üí Einheitliches Interface:

```
OpenAI API (Struktur A) ‚îÄ‚îÄ‚Üí ‚îê
                            ‚îú‚îÄ‚Üí BaseLLMProvider (einheitlich)
Anthropic API (Struktur B) ‚îÄ‚Üí ‚îò
```

**Vorteil:** Client-Code muss nur 1 Interface kennen.

---

## üìö Code-Extraktion: Was brauchst du?

### Minimal (nur Provider-Logik)

```bash
# Kopiere diese Dateien:
core/providers/
‚îú‚îÄ‚îÄ base_provider.py      # Must-have
‚îú‚îÄ‚îÄ types.py              # Must-have
‚îú‚îÄ‚îÄ openai_provider.py    # Optional (nur wenn du OpenAI nutzt)
‚îî‚îÄ‚îÄ anthropic_provider.py # Optional (nur wenn du Claude nutzt)

core/llm_manager.py       # Must-have
```

**Dependencies:**
```txt
pydantic>=2.0.0
httpx>=0.27.0
openai>=1.0.0  # Falls OpenAI
anthropic>=0.18.0  # Falls Anthropic
```

---

### Mit Anpassungen

Wenn du z.B. **keine Pydantic** nutzen willst:

**Ersetze `types.py`:**

```python
# Statt Pydantic BaseModel
from dataclasses import dataclass
from enum import Enum

class Role(str, Enum):
    USER = "user"
    ASSISTANT = "assistant"
    SYSTEM = "system"

@dataclass
class Message:
    role: Role
    content: str

@dataclass
class ModelInfo:
    id: str
    name: str
    provider: str
```

**Funktioniert genauso!**

---

## üöÄ Best Practices

### 1. **Behalte die Abstraktion**

Auch wenn du nur 1 Provider nutzt:

```python
# Schlecht (direkt OpenAI nutzen)
client = OpenAI(...)
response = client.chat.completions.create(...)

# Besser (√ºber Manager)
llm_manager.register_provider("openai", OpenAIProvider(...))
response = await llm_manager.stream_chat(...)
```

**Warum:** Sp√§ter hinzuf√ºgen von Providern ist trivial.

---

### 2. **Trenne UI von Logic**

```
UI-Layer (Flet/Web/CLI/Discord)
    ‚Üì (nutzt)
Core-Layer (LLMManager, Provider)
    ‚Üì (nutzt)
External APIs (OpenAI, Anthropic, etc.)
```

**Vorteil:** UI austauschen ohne Logic zu √§ndern.

---

### 3. **Async/Await konsequent**

```python
# Schlecht (blockierend)
def chat(message):
    response = requests.post(...)  # Blockiert UI!
    return response

# Besser (async)
async def chat(message):
    response = await httpx.post(...)  # Non-blocking
    return response
```

---

### 4. **Error-Handling pro Provider**

```python
async def stream_chat(self, model_id, messages):
    try:
        async for chunk in self.client.stream(...):
            yield chunk
    except ProviderAPIError as e:
        # Provider-spezifischer Fehler
        yield f"Error: {e}"
    except Exception as e:
        # Fallback
        yield f"Unexpected error: {e}"
```

**Vorteil:** Ein Provider-Fehler bringt nicht die ganze App zum Absturz.

---

## üéØ Zusammenfassung

### Was du mitnehmen kannst:

‚úÖ **Code (Python):**
- `core/` komplett kopieren
- In eigene App integrieren (Web, CLI, Bot, ...)

‚úÖ **Patterns (universal):**
- Plugin-System
- Abstract Base Class / Interface
- Dependency Injection
- Strategy & Adapter Pattern

‚úÖ **Konzepte:**
- Separation of Concerns
- Async-First
- Error-Resilience

---

### Typische Use-Cases:

| Deine App | Was √ºbernehmen? | Aufwand |
|-----------|-----------------|---------|
| **FastAPI Web-App** | Core-Logic 1:1 | 30 Min |
| **Discord-Bot** | Core-Logic 1:1 | 20 Min |
| **CLI-Tool** | Core-Logic 1:1 | 15 Min |
| **Eigenes UI-Framework** | Core-Logic, eigene UI | 1-2 Std |
| **Andere Sprache** | Pattern/Konzepte | 3-5 Std |
| **Microservice** | LLMManager als Service | 2-3 Std |

---

## üìñ Weiterf√ºhrend

- **Architektur verstehen**: [06-architecture.md](./06-architecture.md)
- **Provider erstellen**: [03-provider-integration.md](./03-provider-integration.md)
- **Pattern-Theorie**: [00-why-this-app.md](./00-why-this-app.md)

---

**Fragen?** ‚Üí [GitHub Discussions](https://github.com/your-repo/discussions)

**Du hast ein cooles Projekt mit diesem Pattern gebaut?** ‚Üí Zeig es uns! üöÄ
