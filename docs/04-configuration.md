# Konfiguration

Lerne, wie du **KI Chat Pattern** an deine BedÃ¼rfnisse anpassen kannst.

## ðŸ“ Konfigurationsdateien

### `.env` - Umgebungsvariablen

Die `.env`-Datei ist der **zentrale Ort** fÃ¼r API-Keys und sensible Konfiguration.

**Struktur:**
```bash
# .env

# API Keys
OPENAI_API_KEY=sk-proj-...
ANTHROPIC_API_KEY=sk-ant-...
GEMINI_API_KEY=AIza...

# Optionale Konfiguration
OPENAI_BASE_URL=https://api.openai.com/v1  # Custom endpoint
ANTHROPIC_BASE_URL=https://api.anthropic.com  # Custom endpoint
```

**Wichtig:**
- âœ… Wird automatisch geladen beim App-Start
- âœ… Bereits in `.gitignore` (nie committen!)
- âœ… Template verfÃ¼gbar: `.env.example`

### `main.py` - Provider-Konfiguration

Hier aktivierst/deaktivierst du Provider und setzt Defaults:

```python
# main.py

# Standard-Provider festlegen
llm_manager.active_provider_id = "openai"  # Ã„ndere zu deinem bevorzugten Provider
llm_manager.active_model_id = "gpt-4"      # Ã„ndere zu deinem bevorzugten Modell
```

---

## ðŸ”‘ API-Keys verwalten

### Keys sicher speichern

**Beste Methode: .env-Datei**
```bash
# .env
OPENAI_API_KEY=sk-proj-abc123...
ANTHROPIC_API_KEY=sk-ant-xyz789...
```

**Vorteile:**
- âœ… Zentral verwaltbar
- âœ… Einfach zu Ã¤ndern
- âœ… Automatisch ignoriert von Git
- âœ… Funktioniert auf allen Plattformen

### Keys rotieren

**Wann solltest du Keys Ã¤ndern?**
- ðŸ”„ RegelmÃ¤ÃŸig (z.B. alle 90 Tage)
- âš ï¸ Bei SicherheitsvorfÃ¤llen
- ðŸ”“ Wenn Key versehentlich Ã¶ffentlich wurde

**Wie:**
1. Neuen Key beim Provider erstellen
2. `.env`-Datei aktualisieren
3. App neu starten
4. Alten Key beim Provider lÃ¶schen

### Keys fÃ¼r verschiedene Umgebungen

**Entwicklung vs. Produktion:**

**.env.development**
```bash
OPENAI_API_KEY=sk-proj-dev-key-hier
```

**.env.production**
```bash
OPENAI_API_KEY=sk-proj-prod-key-hier
```

**Laden:**
```python
# main.py
from dotenv import load_dotenv
import os

# Lade je nach Umgebung
env = os.getenv("APP_ENV", "development")
load_dotenv(f".env.{env}")
```

---

## ðŸŽ›ï¸ Provider-Konfiguration

### Provider aktivieren/deaktivieren

**In `main.py`:**

```python
# Provider aktivieren
openai_config = ProviderConfig(name="OpenAI", enabled=True)  # âœ… Aktiv
llm_manager.register_provider("openai", openai_provider)

# Provider deaktivieren
anthropic_config = ProviderConfig(name="Anthropic", enabled=False)  # âŒ Inaktiv
# Oder: Einfach auskommentieren
```

### Custom Base URLs

**Use Case:** Eigene API-Proxies, Azure OpenAI, etc.

```python
# OpenAI Ã¼ber Proxy
openai_config = ProviderConfig(
    name="OpenAI",
    base_url="https://my-proxy.example.com/v1"
)

# Azure OpenAI
azure_config = ProviderConfig(
    name="Azure OpenAI",
    base_url="https://your-resource.openai.azure.com",
    api_key=os.getenv("AZURE_OPENAI_KEY")
)
```

### Provider-spezifische Einstellungen

**Beispiel: Timeout erhÃ¶hen**

```python
# In der Provider-Klasse (core/providers/openai_provider.py)

self.client = AsyncOpenAI(
    api_key=api_key,
    base_url=self.config.base_url,
    timeout=60.0  # Default ist 30s
)
```

---

## ðŸŽ¨ UI-Anpassungen

### Theme (aktuell: Dark Mode)

**Wo:** `main.py`, Zeile ~10

```python
async def main(page: ft.Page):
    page.theme_mode = ft.ThemeMode.DARK  # DARK, LIGHT, oder SYSTEM
```

**Optionen:**
- `DARK`: Immer Dark Mode
- `LIGHT`: Immer Light Mode
- `SYSTEM`: Folgt Systemeinstellung

### Fenster-GrÃ¶ÃŸe

```python
async def main(page: ft.Page):
    # Minimale GrÃ¶ÃŸe
    page.window_min_width = 800
    page.window_min_height = 600
    
    # Standard-GrÃ¶ÃŸe beim Start
    page.window_width = 1200
    page.window_height = 800
    
    # Maximiert starten
    page.window_maximized = True
```

### Farben anpassen

**Wo:** In den UI-Komponenten (`ui/` Ordner)

**Beispiel: Sidebar-Farbe Ã¤ndern**

`ui/sidebar.py`, Zeile ~11:
```python
self.bgcolor = "surfaceVariant"  # Ã„ndere zu einer anderen Farbe
```

**VerfÃ¼gbare Farben:**
- Material Design: `"primary"`, `"secondary"`, `"surface"`, etc.
- Hex: `"#1E88E5"`
- RGB: `"rgb(30, 136, 229)"`

---

## âš™ï¸ Erweiterte Einstellungen

### Streaming ein/ausschalten

**Aktuell:** Streaming ist immer aktiv.

**Um es zu deaktivieren** (zeigt komplette Antwort am Ende):

In `core/llm_manager.py`:
```python
async def stream_chat(self, message_history, provider_id=None, model_id=None):
    # Option 1: VollstÃ¤ndige Antwort sammeln
    full_response = ""
    async for chunk in provider.stream_chat(mid, message_history):
        full_response += chunk
    yield full_response  # Nur einmal am Ende
    
    # Option 2: Normal streamen (aktuell)
    async for chunk in provider.stream_chat(mid, message_history):
        yield chunk
```

### System-Prompts

**Was sind System-Prompts?**
Anweisungen an die KI, wie sie sich verhalten soll.

**Beispiel:**
```python
# In ui/app_layout.py, run_chat_flow()

# System-Nachricht hinzufÃ¼gen
system_msg = Message(
    role=Role.SYSTEM,
    content="Du bist ein hilfreicher Assistent der auf Deutsch antwortet."
)
self.message_history.insert(0, system_msg)  # Am Anfang einfÃ¼gen
```

### Model-Parameter (Temperatur, etc.)

**Wo:** In den Provider-Implementierungen

**Beispiel: OpenAI-Provider**

`core/providers/openai_provider.py`, in `stream_chat()`:
```python
stream = await self.client.chat.completions.create(
    model=model_id,
    messages=openai_messages,
    stream=True,
    temperature=0.7,      # KreativitÃ¤t (0.0 - 2.0)
    max_tokens=2000,      # Max. Antwort-LÃ¤nge
    top_p=1.0,           # Nucleus sampling
    frequency_penalty=0,  # Wiederholungen vermeiden
    presence_penalty=0    # Themen-Vielfalt
)
```

**Parameter erklÃ¤rt:**

| Parameter | Wert | Effekt |
|-----------|------|--------|
| `temperature` | 0.0 | Deterministisch, prÃ¤zise |
| | 1.0 | Ausgewogen (Standard) |
| | 2.0 | Sehr kreativ, zufÃ¤llig |
| `max_tokens` | 100 | Kurze Antworten |
| | 2000 | Mittellang (Standard) |
| | 4000+ | Lange Essays |
| `top_p` | 0.1 | Konservativ |
| | 1.0 | Alles mÃ¶glich (Standard) |

---

## ðŸ“Š Logging & Debugging

### Konsolen-Ausgaben aktivieren

**Aktuell:** Warnings und Errors werden ausgegeben.

**Mehr Details:**
```python
# In main.py, am Anfang
import logging

logging.basicConfig(
    level=logging.DEBUG,  # DEBUG, INFO, WARNING, ERROR
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
```

### Provider-Aufrufe loggen

`core/llm_manager.py`:
```python
async def stream_chat(self, message_history, provider_id=None, model_id=None):
    pid = provider_id or self.active_provider_id
    mid = model_id or self.active_model_id
    
    print(f"[DEBUG] Calling {pid} with model {mid}")  # Logging
    print(f"[DEBUG] Message count: {len(message_history)}")
    
    # ... rest des Codes
```

### Fehler-Tracking

**In jedem Provider** (`stream_chat()` Methode):
```python
try:
    async for chunk in stream:
        yield chunk
except Exception as e:
    import traceback
    print(f"[ERROR] {traceback.format_exc()}")  # VollstÃ¤ndiger Stack Trace
    yield f"Error: {e}"
```

---

## ðŸ”’ Sicherheit & Privacy

### API-Keys schÃ¼tzen

**Best Practices:**
1. âœ… Nie keys im Code hardcoden
2. âœ… `.env` in `.gitignore` (bereits gemacht)
3. âœ… Unterschiedliche Keys fÃ¼r Dev/Prod
4. âœ… Keys regelmÃ¤ÃŸig rotieren

**PrÃ¼fen, ob `.env` in Git ist:**
```bash
git status  # .env sollte NICHT auftauchen
```

Falls doch:
```bash
# Aus Git entfernen (nicht vom Filesystem!)
git rm --cached .env
git commit -m "Remove .env from git"
```

### Lokale Daten

**Was wird gespeichert:**
- âœ… API-Keys: Nur in `.env` (nicht im RAM)
- âš ï¸ Chat-History: Aktuell nur im RAM (geht verloren beim SchlieÃŸen)
- âŒ Keine Telemetrie, keine Analytics

**FÃ¼r maximale Privacy:**
- Nutze **Ollama** (100% lokal, kein Internet)
- Oder: Eigene Server mit Self-Hosted LLMs

---

## ðŸ“ Konfigurationsdatei-Referenz

### VollstÃ¤ndige `.env`-Vorlage

```bash
# ============================================
# KI Chat Pattern - Configuration
# ============================================

# ---------------------------------------------
# Provider API Keys
# ---------------------------------------------
OPENAI_API_KEY=sk-proj-...
ANTHROPIC_API_KEY=sk-ant-...
GEMINI_API_KEY=AIza...

# ---------------------------------------------
# Custom Base URLs (Optional)
# ---------------------------------------------
# OPENAI_BASE_URL=https://api.openai.com/v1
# ANTHROPIC_BASE_URL=https://api.anthropic.com
# GEMINI_BASE_URL=https://generativelanguage.googleapis.com

# ---------------------------------------------
# App Settings (Optional)
# ---------------------------------------------
# APP_ENV=development  # development, production
# LOG_LEVEL=INFO       # DEBUG, INFO, WARNING, ERROR
# DEFAULT_PROVIDER=openai
# DEFAULT_MODEL=gpt-4

# ---------------------------------------------
# Advanced (Optional)
# ---------------------------------------------
# API_TIMEOUT=30
# MAX_RETRIES=3
# STREAM_ENABLED=true
```

---

## ðŸš€ Performance-Tuning

### Startup-Zeit reduzieren

**Problem:** App braucht lange zum Starten.

**LÃ¶sung:**
```python
# In main.py: Nur benÃ¶tigte Provider laden

# Statt alle Provider zu laden:
if os.getenv("OPENAI_API_KEY"):
    # Nur laden, wenn Key vorhanden
    openai_provider = OpenAIProvider(openai_config)
    await openai_provider.initialize()
    llm_manager.register_provider("openai", openai_provider)
```

### Streaming-Performance

**Chunk-GrÃ¶ÃŸe anpassen:**

In Provider-Implementierungen (z.B. `openai_provider.py`):
```python
# GrÃ¶ÃŸere Chunks = weniger Updates, schneller
# Kleinere Chunks = flÃ¼ssiger, bessere UX

async for chunk in stream:
    if chunk.choices[0].delta.content:
        # Sammle z.B. 10 Zeichen bevor Update
        buffer += chunk.choices[0].delta.content
        if len(buffer) >= 10:
            yield buffer
            buffer = ""
```

---

## ðŸ“š Weitere Ressourcen

- **Provider-Integration**: [Detaillierter Guide](./03-provider-integration.md)
- **Troubleshooting**: [Probleme lÃ¶sen](./05-troubleshooting.md)
- **Architektur**: [Technische Details](./06-architecture.md)

---

**Fragen zur Konfiguration?** â†’ [Community-Forum](https://github.com/your-repo/discussions)
