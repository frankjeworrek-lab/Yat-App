# Architektur & Entwicklung

Technische Dokumentation fÃ¼r Entwickler, die **KI Chat Pattern** verstehen oder erweitern mÃ¶chten.

## ðŸ—ï¸ System-Architektur

### High-Level Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   UI Layer (Flet)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Sidebar  â”‚  â”‚ChatView  â”‚  â”‚  InputArea       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                      â”‚                              â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚              â”‚  AppLayout     â”‚                     â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Core Logic Layer                       â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚              â”‚ LLMManager  â”‚                        â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                     â”‚                               â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚     â–¼               â–¼               â–¼               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚OpenAI  â”‚   â”‚Anthropicâ”‚    â”‚  Mock    â”‚          â”‚
â”‚  â”‚Providerâ”‚   â”‚Provider â”‚    â”‚ Provider â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚       â”‚             â”‚              â”‚                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚             â”‚              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              External APIs                          â”‚
â”‚    OpenAI API   Anthropic API    Mock Responses    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Design-Prinzipien

1. **Separation of Concerns**
   - UI weiÃŸ nichts Ã¼ber API-Details
   - Provider-Logic isoliert vom Core
   - Core-Logic UI-agnostisch

2. **Async-First**
   - Alle I/O-Operationen sind async
   - Keine UI-Blockierung
   - Streaming ermÃ¶glicht Echtzeit-Updates

3. **Plugin-Architektur**
   - Provider als austauschbare Plugins
   - Neue Provider ohne Core-Ã„nderungen
   - Dependency Injection Pattern

4. **Type-Safety**
   - Pydantic fÃ¼r Datenvalidierung
   - Type Hints Ã¼berall
   - IDE-Support & Auto-Completion

---

## ðŸ“ Code-Struktur

```
ki_chat_pattern/
â”œâ”€â”€ main.py                      # Einstiegspunkt, App-Initialisierung
â”œâ”€â”€ requirements.txt             # Python-Dependencies
â”œâ”€â”€ .env.example                 # Umgebungsvariablen-Template
â”œâ”€â”€ .env                         # Echte Keys (gitignored)
â”‚
â”œâ”€â”€ core/                        # Business Logic
â”‚   â”œâ”€â”€ llm_manager.py          # Provider-Management
â”‚   â””â”€â”€ providers/               # Provider-Implementierungen
â”‚
â”œâ”€â”€ ui/                          # User Interface (Flet)
â”‚   â”œâ”€â”€ app_layout.py           # Main Layout & Orchestration
â”‚   â”œâ”€â”€ sidebar.py              # Model-Selection, Settings
â”‚   â”œâ”€â”€ chat_view.py            # Message Display
â”‚   â””â”€â”€ input_area.py           # User Input
â”‚
â”œâ”€â”€ storage/                     # Persistence Layer
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ chat_db.py              # SQLite Database Logic
â”‚
â”‚       â”œâ”€â”€ types.py            # Shared Types (Pydantic)
â”‚       â”œâ”€â”€ base_provider.py    # Abstract Base Class
â”‚       â”œâ”€â”€ mock_provider.py    # Test-Provider
â”‚       â”œâ”€â”€ openai_provider.py  # OpenAI Integration
â”‚       â”œâ”€â”€ anthropic_provider.py # Anthropic Integration
â”‚       â””â”€â”€ gemini_provider.py  # Google Gemini (Example)
â”‚
â”œâ”€â”€ ui/                          # User Interface (Flet)
â”‚   â”œâ”€â”€ app_layout.py           # Main Layout & Orchestration
â”‚   â”œâ”€â”€ sidebar.py              # Model-Selection, Settings
â”‚   â”œâ”€â”€ chat_view.py            # Message Display
â”‚   â””â”€â”€ input_area.py           # User Input
â”‚
â”œâ”€â”€ config/                      # (Future) Config Management
â”œâ”€â”€ assets/                      # (Future) Images, Icons
â””â”€â”€ docs/                        # Dokumentation
    â”œâ”€â”€ README.md               # Ãœbersicht
    â”œâ”€â”€ 01-getting-started.md
    â”œâ”€â”€ 02-features.md
    â”œâ”€â”€ 03-provider-integration.md
    â”œâ”€â”€ 04-configuration.md
    â”œâ”€â”€ 05-troubleshooting.md
    â””â”€â”€ 06-architecture.md      # This file
```

---

## ðŸ”§ Core-Komponenten

### 1. LLMManager

**Zweck:** Zentrale Verwaltung aller LLM-Provider

**Verantwortlichkeiten:**
- Provider registrieren/verwalten
- Model-Listen aggregieren
- Aktiven Provider/Model tracken
- Stream-Aufrufe orchestrieren

**API:**
```python
class LLMManager:
    def register_provider(provider_id: str, provider: BaseLLMProvider)
    async def get_all_models() -> List[ModelInfo]
    async def stream_chat(messages: List[Message], ...) -> AsyncGenerator[str]
```

**Beispiel-Flow:**
```python
# 1. Provider registrieren
manager = LLMManager()
manager.register_provider("openai", OpenAIProvider(config))

# 2. Models abfragen
models = await manager.get_all_models()  # Aggregiert von allen Providern

# 3. Chat-Stream
async for chunk in manager.stream_chat(message_history):
    print(chunk)  # Echtzeit-Ausgabe
```

---

### 2. BaseLLMProvider (ABC)

**Zweck:** Contract fÃ¼r alle Provider-Implementierungen

**Interface:**
```python
class BaseLLMProvider(ABC):
    @abstractmethod
    async def initialize() -> None
        """Setup: API-Client initialisieren, Keys validieren"""
    
    @abstractmethod
    async def get_models() -> List[ModelInfo]
        """Model-Discovery: Welche Modelle sind verfÃ¼gbar?"""
    
    @abstractmethod
    async def stream_chat(...) -> AsyncGenerator[str, None]
        """Streaming: Sende Messages, empfange Chunks"""
    
    @abstractmethod
    async def check_health() -> bool
        """Health-Check: Ist Service erreichbar?"""
```

**Implementierungs-Beispiel:**
```python
class CustomProvider(BaseLLMProvider):
    async def initialize(self) -> None:
        self.client = CustomAPI(api_key=self.config.api_key)
    
    async def get_models(self) -> List[ModelInfo]:
        return [ModelInfo(id="custom-1", name="Custom Model", ...)]
    
    async def stream_chat(self, model_id, messages, **kwargs):
        for word in "Hello from custom provider".split():
            await asyncio.sleep(0.1)  # Simulate delay
            yield word + " "
    
    async def check_health(self) -> bool:
        return self.client is not None
```

---

### 3. Pydantic Models (types.py)

**Zweck:** Type-Safety und Datenvalidierung

**Models:**

```python
class Role(str, Enum):
    """Message role"""
    USER = "user"
    ASSISTANT = "assistant"
    SYSTEM = "system"

class Message(BaseModel):
    """Single chat message"""
    role: Role
    content: str
    timestamp: datetime = Field(default_factory=datetime.now)
    metadata: Dict[str, Any] = Field(default_factory=dict)

class ModelInfo(BaseModel):
    """Model metadata"""
    id: str
    name: str
    provider: str
    capabilities: List[ModelCapability] = [ModelCapability.CHAT]
    context_window: Optional[int] = None
    max_tokens: Optional[int] = None
    provider_id: Optional[str] = None  # Injected by LLMManager

class ProviderConfig(BaseModel):
    """Provider configuration"""
    name: str
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    enabled: bool = True
```

**Vorteile:**
- Auto-Validation (z.B. role muss USER|ASSISTANT|SYSTEM sein)
- IDE-Autocomplete
- JSON-Serialisierung out-of-the-box
- Dokumentation durch Types

---

### 4. Persistence Layer (storage/chat_db.py)

**Zweck:** Dauerhafte Speicherung von Chats und Messages

**Technologie:** `sqlite3` (Built-in, keine Extra-Dependency)

**Datenbank-Schema:**
```sql
CREATE TABLE conversations (
    id TEXT PRIMARY KEY,
    title TEXT,
    updated_at TEXT,
    ...
);

CREATE TABLE messages (
    id INTEGER PRIMARY KEY,
    conversation_id TEXT,
    role TEXT,
    content TEXT,
    timestamp TEXT,
    metadata TEXT
);
```

**Interface:**
```python
class ChatDatabase:
    def create_conversation(self, ...) -> str
    def save_message(self, conversation_id, message: Message)
    def load_messages(self, conversation_id) -> List[Message]
    def get_conversations(self, limit=50) -> List[Dict]
```

---

## ðŸŽ¨ UI-Architektur

### Component-Hierarchie

```
AppLayout (ft.Row)
â”‚
â”œâ”€ Sidebar (ft.Container)
â”‚  â”œâ”€ ft.Text("KI Chat")
â”‚  â”œâ”€ ft.Dropdown (Model Selection)
â”‚  â””â”€ ft.ElevatedButton (Settings)
â”‚
â”œâ”€ ft.VerticalDivider
â”‚
â””â”€ Main Content (ft.Container)
   â”œâ”€ ChatView (ft.Container)
   â”‚  â””â”€ ft.ListView (Messages)
   â”‚     â”œâ”€ ft.Row (Message 1)
   â”‚     â”œâ”€ ft.Row (Message 2)
   â”‚     â””â”€ ...
   â”‚
   â””â”€ InputArea (ft.Container)
      â””â”€ ft.Row
         â”œâ”€ ft.TextField
         â””â”€ ft.IconButton (Send)
```

### State Management

**Aktuell:** Component-lokaler State

```python
class AppLayout:
    def __init__(self, page, llm_manager):
        self.main_page = page
        self.llm_manager = llm_manager
        self.message_history: List[Message] = []  # App State
```

**ZukÃ¼nftig:** Evtl. zentraler State-Manager (Redux-Style)

---

## ðŸ”„ Datenfluss

### Message-Send-Flow

```
User tippt Nachricht â†’ Enter/Send-Button
    â†“
InputArea.handle_submit()
    â†“
AppLayout.handle_input_submit()
    â†“
AppLayout.run_chat_flow() [async]
    â†“
â”œâ”€ User-Message zu History hinzufÃ¼gen
â”œâ”€ ChatView.add_message(user_msg)
â”œâ”€ Placeholder-AI-Message erstellen
â”œâ”€ ChatView.add_message(assistant_msg)
    â†“
LLMManager.stream_chat(history)
    â†“
Provider.stream_chat(model_id, messages)
    â†“
[Streaming-Loop]
    â”œâ”€ Chunk empfangen
    â”œâ”€ ChatView.update_last_message(accumulated_content)
    â””â”€ Repeat
    â†“
Final: assistant_msg.content = full_response
```

### Async-Pattern

**Wichtig:** UI-Callbacks (wie `on_submit`) kÃ¶nnen nicht direkt async sein.

**LÃ¶sung:** `asyncio.create_task()`

```python
def handle_input_submit(self, text):
    # Synchroner Callback
    asyncio.create_task(self.run_chat_flow(text))  # Async Task starten

async def run_chat_flow(self, text):
    # Async-Logic hier
    async for chunk in self.llm_manager.stream_chat(...):
        self.chat_view.update_last_message(...)
```

---

## ðŸ› ï¸ Entwicklungs-Workflows

### Neuen Provider hinzufÃ¼gen

**Schritt 1:** Provider-Klasse erstellen
```bash
touch core/providers/my_provider.py
```

**Schritt 2:** BaseLLMProvider implementieren
```python
from .base_provider import BaseLLMProvider

class MyProvider(BaseLLMProvider):
    # Implementiere alle abstract methods
    pass
```

**Schritt 3:** In `main.py` registrieren
```python
from core.providers.my_provider import MyProvider

my_config = ProviderConfig(name="My Service")
my_provider = MyProvider(my_config)
await my_provider.initialize()
llm_manager.register_provider("my_provider", my_provider)
```

**Fertig!** Model erscheint im Dropdown.

---

### Neues UI-Feature hinzufÃ¼gen

**Beispiel:** "Clear Chat" Button

**Schritt 1:** UI-Element in Sidebar
```python
# ui/sidebar.py

clear_button = ft.ElevatedButton(
    "Clear Chat",
    on_click=self.handle_clear_chat
)

# In Column.controls hinzufÃ¼gen
```

**Schritt 2:** Callback implementieren
```python
# ui/sidebar.py

def handle_clear_chat(self, e):
    # Callback zu AppLayout
    if self.on_clear_chat:
        self.on_clear_chat()
```

**Schritt 3:** Logic in AppLayout
```python
# ui/app_layout.py

self.sidebar = Sidebar(llm_manager, on_clear_chat=self.handle_clear_chat)

def handle_clear_chat(self):
    self.message_history.clear()
    self.chat_view.clear()
```

---

### Testing

**Aktuell:** Keine automatischen Tests

**Geplant:**
```python
# tests/test_providers.py

import pytest
from core.providers.mock_provider import MockProvider

@pytest.mark.asyncio
async def test_mock_provider_streaming():
    provider = MockProvider(ProviderConfig(name="Test"))
    await provider.initialize()
    
    messages = [Message(role=Role.USER, content="Hello")]
    chunks = []
    
    async for chunk in provider.stream_chat("mock-gpt-4", messages):
        chunks.append(chunk)
    
    assert len(chunks) > 0
    assert "mocked response" in "".join(chunks)
```

**Test-Setup:**
```bash
pip install pytest pytest-asyncio
pytest tests/
```

---

## ðŸ“¦ Dependencies

### Production

| Package | Version | Zweck |
|---------|---------|-------|
| `flet` | >=0.21.0 | UI Framework |
| `pydantic` | >=2.0.0 | Data Validation |
| `httpx` | >=0.27.0 | Async HTTP Client |
| `python-dotenv` | >=1.0.0 | .env Loading |
| `openai` | >=1.0.0 | OpenAI SDK |
| `anthropic` | >=0.18.0 | Anthropic SDK |

### Optional

| Package | Zweck |
|---------|-------|
| `google-generativeai` | Gemini Support |
| `pytest` | Testing |
| `black` | Code Formatting |
| `mypy` | Type Checking |

---

## ðŸš€ Performance-Optimierungen

### Lazy Loading

Provider nur laden, wenn Keys vorhanden:
```python
if os.getenv("OPENAI_API_KEY"):
    from core.providers.openai_provider import OpenAIProvider
    # ...
```

### Caching

Model-Listen cachen (1 API-Call statt N):
```python
class LLMManager:
    def __init__(self):
        self._model_cache: Optional[List[ModelInfo]] = None
        self._cache_time: Optional[datetime] = None
    
    async def get_all_models(self):
        if self._model_cache and (datetime.now() - self._cache_time).seconds < 300:
            return self._model_cache  # Cache hit
        
        # Cache miss: Fresh fetch
        self._model_cache = await self._fetch_models()
        self._cache_time = datetime.now()
        return self._model_cache
```

### UI-Update-Batching

Nicht jedes Chunk einzeln rendern:
```python
buffer = ""
for i, chunk in enumerate(chunks):
    buffer += chunk
    if i % 10 == 0:  # Alle 10 chunks
        self.chat_view.update_last_message(buffer)
```

---

## ðŸ” Sicherheits-Ãœberlegungen

### API-Key-Handling

**DO:**
- âœ… Keys in `.env` speichern
- âœ… `.env` in `.gitignore`
- âœ… `os.getenv()` verwenden
- âœ… Keys verschlÃ¼sselt speichern (zukÃ¼nftig)

**DON'T:**
- âŒ Keys in Code hardcoden
- âŒ Keys in Logs ausgeben
- âŒ Keys in Exception-Messages

### Input-Validation

Pydantic validiert automatisch:
```python
# Wirft ValidationError bei falschem Type
msg = Message(role="invalid", content="...")  # âŒ Fehler

msg = Message(role=Role.USER, content="...")  # âœ… OK
```

### Rate-Limiting

**Aktuell:** Nicht implementiert

**ZukÃ¼nftig:**
```python
from ratelimit import limits

@limits(calls=10, period=60)  # 10 calls/minute
async def stream_chat(...):
    pass
```

---

## ðŸ”® Zukunfts-Features

### Persistente History

```python
# Vorgeschlagen: SQLite
import sqlite3

class HistoryManager:
    def save_conversation(self, messages: List[Message]):
        # Save to DB
        pass
    
    def load_conversation(self, conversation_id: str):
        # Load from DB
        pass
```

### Multi-Turn-Context

Automatisches Context-Window Management:
```python
def trim_history(messages, max_tokens=4000):
    """Keep only recent messages that fit in context"""
    # Token counting logic
    pass
```

### Plugin-System

```python
# plugins/sentiment_analysis.py
class SentimentPlugin(BasePlugin):
    def before_send(self, message):
        # Analyze vor dem Senden
        pass
    
    def after_receive(self, response):
        # Process Antwort
        pass
```

---

## ðŸ“š Weitere Ressourcen

- **Getting Started**: [Installation & Setup](./01-getting-started.md)
- **Provider Guide**: [Provider hinzufÃ¼gen](./03-provider-integration.md)
- **Troubleshooting**: [HÃ¤ufige Probleme](./05-troubleshooting.md)

---

## ðŸ¤ Contribution-Guidelines

**Interessiert mitzuarbeiten?**

1. **Fork** das Repo
2. **Branch** erstellen: `git checkout -b feature/my-feature`
3. **Code** schreiben (mit Type-Hints!)
4. **Tests** hinzufÃ¼gen (wenn relevant)
5. **Commit**: `git commit -m "Add: My Feature"`
6. **Push**: `git push origin feature/my-feature`
7. **Pull Request** erstellen

**Code-Style:**
- Black fÃ¼r Formatting
- Type-Hints Ã¼berall
- Docstrings fÃ¼r Public APIs

---

**Happy Coding!** ðŸš€
